---
layout: post
tag: OpenStack
date: '\[2015-07-09 四 13:15\]'
setupfile: '\~/Dropbox/Doc/Org\_Templates/level-1.org'
title: Cinder Private Volume Type(租户卷类型) 在大规模部署中带来的好处
---

问题的提出
==========

在 OpenStack
部署中，一般将不同存储类型，不同性能规格的存储资源形成一个大的集群，对外提供云硬盘的服务，这个大的集群统一由
cinder 来管理。但是在实践中，往往会遇到一些问题。

我们知道所有的计算节点可以按照 Zone
来划分，把计算机点按机房(机柜)的位置纳入到一个 Zone
中，如果其中一个机房(机柜)因为某种原因出问题了，不会影响其它机房/柜的虚拟机和网络。
而且, 同属于一个 Zone 的云主机在发生租户内部的流量的时候，就不会超出这个
Zone 的网络，租户网络的性能得到最大的利用和提升。

不过对于存储的访问，如果没有类似于这种隔离的技术，就会造成存储网络不可控，存储流量在各个存储交换机之间乱跑。

另一方面，根据不同的要求往往划分为多种不同的类型的云硬盘：例如:

-   根据不同的后端存储
-   创建不同性能的云硬盘
-   为了开发新技术，创建一些测试类型的存储卷

但是所有的存储类型对租户都是可见的，用户创建磁盘的时候可以任意选择云硬盘类型，哪怕这些类型并不希望被某些用户使用。更糟糕的是，用户创建基于新的云硬盘启动的云主机的时候，这个新的云硬盘的类型是随机的！

情况在 Kilo 版本带来的名为 Private Type(私有卷)
的功能得到改善。该功能允许创建租户私有的云硬盘类型，这些类型的云硬盘指定为只能被某些租户访问。

解决方式
========

综上，对之前提到的两个问题，就有了比较好的解决方案。下面以使用一个 Ceph
作为存储后端为例说明：

-   首先，从物理层面上，将一个 Zone 内的计算节点的存储网络和适当的 Ceph
    节点的存储网络放在同一个网络设备下（比如同一个交换机）。

-   然后，创建一个 Ceph 集群的资源池，指定资源池包含上面的 Ceph 节点。

-   最后，根据上面的资源池创建对应的的云硬盘类型，并设置改类型为私有的，将它的访问权限赋予适当的租户。

至此，没有被赋予访问权限的租户将看不到这些存储。完成了实际上的存储隔离。

需要注意的地方
==============

注意：即使是 Kilo 版本的 OpenStack，也没有完善该功能，
需要对创建云硬盘部分进行改动，确保在 nova
创建基于云硬盘启动的云主机的时候，只能创建具有权限的云主机。

参考
====

具体改功能的用法请参考 [这里](http://mathslinux.org/?p%3D657) 的干货
